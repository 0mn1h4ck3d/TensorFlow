{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tr41z/machine-learning/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI5VD13M8fY6"
      },
      "source": [
        "# Get Data and import libraries that we need"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version of this jupyter notebook: sentimentanalysis_V2.4"
      ],
      "metadata": {
        "id": "eMwtYRl71geH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LdLsVQjs8R-X"
      },
      "outputs": [],
      "source": [
        "# Define where we will download the data\n",
        "path_data = \"/content/sample_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_sKSOI8uZJ"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cSPQApSIqEL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a46472-4d05-4330-caf4-58f56f06f5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-07 11:37:44--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘/content/sample_data/aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  32.8MB/s    in 2.4s    \n",
            "\n",
            "2024-11-07 11:37:46 (32.8 MB/s) - ‘/content/sample_data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -P /content/sample_data/ -c \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Ka_9Fk80iw"
      },
      "source": [
        "Decompress the archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HyPwL6EYq-ej"
      },
      "outputs": [],
      "source": [
        "!tar -xf  /content/sample_data/aclImdb_v1.tar.gz -C /content/sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mksTVcHZ9MQ-"
      },
      "source": [
        "Check that the folder aclImdb exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xt0S2EZErSi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c57063-fe8b-4aaf-d9d6-98e1fe4f4a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb\t\t   anscombe.json\t\tcalifornia_housing_train.csv  mnist_train_small.csv\n",
            "aclImdb_v1.tar.gz  california_housing_test.csv\tmnist_test.csv\t\t      README.md\n"
          ]
        }
      ],
      "source": [
        "!ls /content/sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZpg6f1u9W76"
      },
      "source": [
        "Import all required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5bZ5Cng09LEl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wytbs1XU9zgG"
      },
      "source": [
        "Define function that reads all the files from a given folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zEpWeGRq2QYd"
      },
      "outputs": [],
      "source": [
        "def read_data(path, files):\n",
        "  data = []\n",
        "  for f in files:\n",
        "    with open(path+f) as file:\n",
        "      ### BEGIN YOUR CODE HERE\n",
        "      ## Read a line from the file and append it to the data list variable.\n",
        "      ## TIP: use function append(): https://www.w3schools.com/python/ref_list_append.asp\n",
        "      ## and readline(): https://www.w3schools.com/python/ref_file_readline.asp\n",
        "      line = file.readline()\n",
        "      data.append(line)\n",
        "\n",
        "\n",
        "      ### END YOUR CODE HERE\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zF9Dz-E-NQv"
      },
      "source": [
        "Load data (movie reviews), both for train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oRmD7SSBrr71"
      },
      "outputs": [],
      "source": [
        "path_data_train_pos = path_data + '/aclImdb/train/pos/'\n",
        "path_data_train_neg = path_data + '/aclImdb/train/neg/'\n",
        "path_data_test_pos = path_data + '/aclImdb/test/pos/'\n",
        "path_data_test_neg = path_data + '/aclImdb/test/neg/'\n",
        "\n",
        "# Get the list of files from the four folders\n",
        "train_pos_files = os.listdir(path_data_train_pos)\n",
        "train_neg_files = os.listdir(path_data_train_neg)\n",
        "test_pos_files = os.listdir(path_data_test_pos)\n",
        "test_neg_files = os.listdir(path_data_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l-OHWFG92TV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49019196-52a0-4c61-e410-e1134434cf89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive training: 12500, Negative training: 12500\n",
            "Positive testing: 12500, Negative testing: 12500\n",
            "==============================\n",
            "Train data positive example:\n",
            "The \"movie aimed at adults\" is a rare thing these days, but Moonstruck does it well, and is still a better than average movie, which is aging very well. Although it's comic moments aim lower than the rest of it, the movie has a wonderful specificity (Italians in Brooklyn) that isn't used to shortchange the characters or the viewers. (i.e. Mobsters never appear in acomplication. It never becomes grotesque like My Big Fat Greek Wedding) The secondary story lines are economically told with short scenes that allow a break from the major thread. These are the scenes that are now missing in contemporary movies where their immediate value cannot be impressed upon producers and bigwigs. I miss these scenes. It also beautifully involves older characters. The movie takes it's own slight, quiet path to a conclusion. There isn't a poorly written scene included anywhere to make some executives sphincter relax. Cage and Cher do very nice work.<br /><br />Moonstruck invokes old-school, ethnic, workaday New York much like 'Marty' except Moonstruck is way less sanctimonious.\n",
            "\n",
            "==============================\n",
            "Train data negative example:\n",
            "When the young Kevin gets the boat of his dead uncle as a gift, he invites five friends of him to a trip to Catalina Island for the weekend. While in the journey, they drink booze, have sex and play games, with each one of them telling his or her greatest fear. Later Kevin drowns in the open sea, the engine stops, and they are haunted and murdered by their greatest innermost fear.<br /><br />Yesterday, my wife, son, daughter and three other friends joined to watch \"Haunted Boat\" on DVD. With less than 30 minutes running time, the group gave up watching this messy and boring amateurish piece of crap, and we decided to see another film. Later, I decided to watch the rest of this flick to see how bad it could be and it would have been better off going to bed to sleep. The confused story has an awful cinematography and camera work, with a cast that is probably studying to be actors and actresses and in the end this film seems to be a bad project of cinema school. The terrible and pretentious screenplay shows a ridiculous twist in the end, actually a complete mess that made me not understand what the story is all about. Was the girl insane and traveled alone in the boat, imagining the whole situation with imaginary friends? If that is true, are their friend again in the very end fruit of her madness? My vote is one.<br /><br />Title (Brazil): \"Viagem Para a Morte\" (\"Trip to the Death\")\n",
            "\n",
            "==============================\n",
            "Test data positive example:\n",
            "This is the last episode of the Goldenboy OVA series. Kentaro finds himself working in an animation studio, which is rather interesting if you don't know anything about the way anime studios were run. Besides episode 3, this was probably the least risqué, but it had a nice girl interest, as well as a surprise reunion from others in the previous episodes. My only complaint about this episode is it seemed a little too short, but at the same time this may have only been because it was the only original script for the show that wasn't based on one of the manga chapters. but it ended well, leaving us with the nice feeling that Kentaro is permanently 25, studying on. Definitely watch the rest of the series all the way through, you can buy the whole series for like $17, you can watch it all the way through in about 2 1/2 hours, or watch your favorite episode if you have 20 minutes free time (which i do if i have a lunch break at school.) good series, check it out.\n",
            "\n",
            "==============================\n",
            "Test data negative example:\n",
            "The 70s were without a doubt the golden age of \"made-for-TV\" horror. This is one of those that was probably churned out as an ABC movie of the week. It's the old story of a house and/or ghost possessing one of the new owners. The movie has promise, but it's never realized. Everything is rushed too much. Tension and suspense need time to build and grow. And there's nothing new. We've seen it all before and one better.<br /><br />One final thought: I don't understand why Robert Wagner's character would fall for the long dead movie star when he's married to Kate Jackson for goodness sake!!!\n"
          ]
        }
      ],
      "source": [
        "### BEGIN YOUR CODE HERE\n",
        "## Read the review data in these four variables: train_data_pos, train_data_neg,\n",
        "## test_data_pos and test_data_neg using the function defined above read_data().\n",
        "## Tip: First argument is the folder containing the files, second argument is\n",
        "# the list of files\n",
        "\n",
        "train_data_pos = read_data(path_data_train_pos, train_pos_files)\n",
        "train_data_neg = read_data(path_data_train_neg, train_neg_files)\n",
        "test_data_pos = read_data(path_data_test_pos, test_pos_files)\n",
        "test_data_neg = read_data(path_data_test_neg, test_neg_files)\n",
        "\n",
        "# 1a. How many examples do we have in training for positive reviews? How many for negative review?\n",
        "# positive - 12500\n",
        "# negative - 12500\n",
        "print(f\"Positive training: {len(train_data_pos)}, Negative training: {len(train_data_neg)}\")\n",
        "\n",
        "\n",
        "# 1b. How about in the testing set?\n",
        "# positive - 12500\n",
        "# negative - 12500\n",
        "print(f\"Positive testing: {len(test_data_pos)}, Negative testing: {len(test_data_neg)}\")\n",
        "\n",
        "\n",
        "\n",
        "# 1c. Is the dataset balanced or not?\n",
        "# Yes\n",
        "\n",
        "# 2.Print examples of positive and negative reviews from training and testing dataset\n",
        "# Tip: the output of the read_data() function is a list.\n",
        "# https://www.w3schools.com/python/python_lists_access.asp\n",
        "print(\"==============================\")\n",
        "print(\"Train data positive example:\")\n",
        "print(train_data_pos[0])\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"Train data negative example:\")\n",
        "print(train_data_neg[0])\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"Test data positive example:\")\n",
        "print(test_data_pos[0])\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"Test data negative example:\")\n",
        "print(test_data_neg[0])\n",
        "\n",
        "\n",
        "### END YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ONyrIyabhfx_"
      },
      "outputs": [],
      "source": [
        "# Let's work on a subset of training and testing dataset to start with.\n",
        "# I recommend that while you are developing the code to work with a small number of examples, maybe 1000.\n",
        "# This will speed up how fast you can get the actual results. After the code is working, for the full tests\n",
        "# please use the entire dataset\n",
        "\n",
        "### BEGIN YOUR CODE HERE\n",
        "sample_number = 1000\n",
        "### END YOUR CODE HERE\n",
        "train_data_pos = train_data_pos[:sample_number]\n",
        "train_data_neg = train_data_neg[:sample_number]\n",
        "test_data_pos = test_data_pos[:sample_number]\n",
        "test_data_neg = test_data_neg[:sample_number]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSNchHyBSiP"
      },
      "source": [
        "Create the data structures that we'll use for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-iRunxcO214k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9feb60b-d196-4dee-9d78-9b1bfea2f01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the positive training examples is 1000\n",
            "Length of the negative training examples is 1000\n",
            "Length of the positive testing examples is 1000\n",
            "Length of the negative testing examples is 1000\n"
          ]
        }
      ],
      "source": [
        "### BEGIN YOUR CODE HERE\n",
        "# Assign the length of the train_data_pos to variable length_train_pos\n",
        "# Tip: to get the length of an array, you can use the function len()\n",
        "length_train_pos = len(train_data_pos)\n",
        "\n",
        "# Assign the length of the train_data_neg to variable length_train_neg\n",
        "length_train_neg = len(train_data_neg)\n",
        "\n",
        "# Assign the length of the test_data_pos to variable length_test_pos\n",
        "length_test_pos = len(test_data_pos)\n",
        "\n",
        "# Assign the length of the test_data_neg to variable length_test_neg\n",
        "length_test_neg = len(test_data_neg)\n",
        "\n",
        "### END YOUR CODE HERE\n",
        "\n",
        "print(\"Length of the positive training examples is\", length_train_pos )\n",
        "print(\"Length of the negative training examples is\", length_train_neg)\n",
        "print(\"Length of the positive testing examples is\", length_test_pos )\n",
        "print(\"Length of the negative testing examples is\", length_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SP0iN7Y63DK8"
      },
      "outputs": [],
      "source": [
        "# Create the training DataFrame with examples and labels\n",
        "# Concatenate positive and negative training examples, then pair each example with a label: 1 for positive, 0 for negative\n",
        "data_train = pd.DataFrame(zip(train_data_pos+train_data_neg, [1]*length_train_pos+[0]*length_train_neg),  columns=['review', 'label'])\n",
        "# Create the test DataFrame with examples and labels\n",
        "# Concatenate positive and negative test examples, then pair each example with a label: 1 for positive, 0 for negative\n",
        "data_test = pd.DataFrame(zip(test_data_pos+test_data_neg, [1]*length_test_pos+[0]*length_test_pos),  columns=['review', 'label'])\n",
        "# Combine all reviews into a single list (training + test, positive + negative)\n",
        "all_reviews = train_data_pos+train_data_neg+test_data_pos+test_data_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-AgnWZ6xmPKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a213bc34-05bf-4ac9-f6d7-3637b7d0a2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the train reviews is 2000\n",
            "The length of the test reviews is 2000\n",
            "The length of all reviews is 4000\n"
          ]
        }
      ],
      "source": [
        "print(\"The length of the train reviews is\",len(data_train))\n",
        "print(\"The length of the test reviews is\",len(data_test))\n",
        "print(\"The length of all reviews is\",len(all_reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGx0YdcMDKmk"
      },
      "source": [
        "## The following are functions for preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_kUEiSjwDWtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17e3cdc-0676-471f-d423-d37f032c1bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords and wordnet vectors\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "counter = 0\n",
        "REPLACE_WITH_SPACE = re.compile(r'[^A-Za-z\\s]')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "# Declare the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "upAJm0cL4m55"
      },
      "outputs": [],
      "source": [
        "# The following functions preprocess text data\n",
        "def clean_review(raw_review: str) -> str:\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(str(raw_review), \"lxml\").get_text()\n",
        "    # 2. Remove non-letters\n",
        "    letters_only = REPLACE_WITH_SPACE.sub(\" \", review_text)\n",
        "    # 3. Convert to lower case\n",
        "    lowercase_letters = letters_only.lower()\n",
        "    return lowercase_letters\n",
        "\n",
        "\n",
        "def lemmatize(tokens: list) -> list:\n",
        "    # 1. Lemmatize\n",
        "    tokens = list(map(lemmatizer.lemmatize, tokens))\n",
        "    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n",
        "    # 2. Remove stop words\n",
        "    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n",
        "    return meaningful_words\n",
        "\n",
        "\n",
        "def preprocess(review: str, total: int, show_progress: bool = True) -> list:\n",
        "    if show_progress:\n",
        "        global counter\n",
        "        counter += 1\n",
        "        print('Processing... %6i/%6i'% (counter, total), end='\\r')\n",
        "    # 1. Clean text\n",
        "    review = clean_review(review)\n",
        "    # 2. Split into individual words\n",
        "    tokens = word_tokenize(review)\n",
        "    # 3. Lemmatize\n",
        "    lemmas = lemmatize(tokens)\n",
        "    # 4. Join the words back into one string separated by space,\n",
        "    # and return the result.\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZheAzAmxEIRi"
      },
      "source": [
        "## Preprocess the text of the reviews by removing the non-word characters, converting everything to lower case, and lemmatizing words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6qCpWwk299fJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae2c15a-2727-40f6-c43b-60d199533fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 'review' column first row:\n",
            "The \"movie aimed at adults\" is a rare thing these days, but Moonstruck does it well, and is still a better than average movie, which is aging very well. Although it's comic moments aim lower than the rest of it, the movie has a wonderful specificity (Italians in Brooklyn) that isn't used to shortchange the characters or the viewers. (i.e. Mobsters never appear in acomplication. It never becomes grotesque like My Big Fat Greek Wedding) The secondary story lines are economically told with short scenes that allow a break from the major thread. These are the scenes that are now missing in contemporary movies where their immediate value cannot be impressed upon producers and bigwigs. I miss these scenes. It also beautifully involves older characters. The movie takes it's own slight, quiet path to a conclusion. There isn't a poorly written scene included anywhere to make some executives sphincter relax. Cage and Cher do very nice work.<br /><br />Moonstruck invokes old-school, ethnic, workaday New York much like 'Marty' except Moonstruck is way less sanctimonious.\n",
            "\n",
            "\n",
            "Training 'label' column first row:\n",
            "1\n",
            "\n",
            "\n",
            "Preprocessed text:\n",
            "['movie', 'aim', 'adult', 'rare', 'thing', 'day', 'moonstruck', 'doe', 'well', 'still', 'better', 'average', 'movie', 'age', 'well', 'although', 'comic', 'moment', 'aim', 'lower', 'rest', 'movie', 'ha', 'wonderful', 'specificity', 'italian', 'brooklyn', 'use', 'shortchange', 'character', 'viewer', 'e', 'mobster', 'never', 'appear', 'acomplication', 'never', 'become', 'grotesque', 'like', 'big', 'fat', 'greek', 'wed', 'secondary', 'story', 'line', 'economically', 'tell', 'short', 'scene', 'allow', 'break', 'major', 'thread', 'scene', 'miss', 'contemporary', 'movie', 'immediate', 'value', 'impress', 'upon', 'producer', 'bigwig', 'miss', 'scene', 'also', 'beautifully', 'involve', 'older', 'character', 'movie', 'take', 'slight', 'quiet', 'path', 'conclusion', 'poorly', 'write', 'scene', 'include', 'anywhere', 'make', 'executive', 'sphincter', 'relax', 'cage', 'cher', 'nice', 'work', 'moonstruck', 'invoke', 'old', 'school', 'ethnic', 'workaday', 'new', 'york', 'much', 'like', 'marty', 'except', 'moonstruck', 'way', 'le', 'sanctimonious']\n"
          ]
        }
      ],
      "source": [
        "### Begin your code here\n",
        "\n",
        "# Display the first review text from the training data.\n",
        "# Purpose: To check how the raw data looks before applying any preprocessing.\n",
        "# Tip: The training data is stored in a pandas DataFrame (2-dimensional array-like structure).\n",
        "# Here, we access the 'review' column of the first row.\n",
        "print(\"Training 'review' column first row:\")\n",
        "print(data_train['review'].iloc[0])\n",
        "\n",
        "# Display the label associated with the first review in the training data.\n",
        "# Purpose: To confirm the label for the first review, indicating its classification (1 for positive, 0 for negative).\n",
        "print(\"\\n\\nTraining 'label' column first row:\")\n",
        "print(data_train['label'].iloc[0])\n",
        "\n",
        "# Display the preprocessed text of the first review.\n",
        "# Purpose: To see the effect of preprocessing on the raw text.\n",
        "# Tip: Use the preprocess() function, passing the first review text and specifying \"1\" as the second argument\n",
        "# since we're processing a single example.\n",
        "preprocessed_text = preprocess(data_train['review'].iloc[0], 1, show_progress=False)\n",
        "print(\"\\n\\nPreprocessed text:\")\n",
        "print(preprocessed_text)\n",
        "\n",
        "### End your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8v52gEBEtUi"
      },
      "source": [
        "Let's preproces the entire set of reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EhjaSx_R_QUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fab4b1f-4042-4bd9-9638-115f2caebdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing...      4/  4000\rProcessing...      5/  4000\rProcessing...      6/  4000\rProcessing...      7/  4000\rProcessing...      8/  4000\rProcessing...      9/  4000\rProcessing...     10/  4000\rProcessing...     11/  4000\rProcessing...     12/  4000\rProcessing...     13/  4000\rProcessing...     14/  4000\rProcessing...     15/  4000\rProcessing...     16/  4000\rProcessing...     17/  4000\rProcessing...     18/  4000\rProcessing...     19/  4000\rProcessing...     20/  4000\rProcessing...     21/  4000\rProcessing...     22/  4000\rProcessing...     23/  4000\rProcessing...     24/  4000\rProcessing...     25/  4000\rProcessing...     26/  4000\rProcessing...     27/  4000\rProcessing...     28/  4000\rProcessing...     29/  4000\rProcessing...     30/  4000\rProcessing...     31/  4000\rProcessing...     32/  4000\rProcessing...     33/  4000\rProcessing...     34/  4000\rProcessing...     35/  4000\rProcessing...     36/  4000\rProcessing...     37/  4000\rProcessing...     38/  4000\rProcessing...     39/  4000\rProcessing...     40/  4000\rProcessing...     41/  4000\rProcessing...     42/  4000\rProcessing...     43/  4000\rProcessing...     44/  4000\rProcessing...     45/  4000\rProcessing...     46/  4000\rProcessing...     47/  4000\rProcessing...     48/  4000\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-0ef2d112b970>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  review_text = BeautifulSoup(str(raw_review), \"lxml\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "# Preprocess each review in all_reviews, applying the preprocess function to each item.\n",
        "# The preprocess function  takes each review (x) and the total number of reviews (len(all_reviews))\n",
        "# as inputs, performing some transformation or cleaning on each review based on the dataset size.\n",
        "\n",
        "all_reviews = [preprocess(x, len(all_reviews)) for x in all_reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9o9C3aCI8TiF"
      },
      "outputs": [],
      "source": [
        "# Slice the preprocessed reviews to get only the training set portion (first len(train_data_pos) + len(train_data_neg) items)\n",
        "X_train_preprocessed = all_reviews[:(len(train_data_pos)+len(train_data_neg))]\n",
        "X_test_preprocessed = all_reviews[(len(train_data_pos)+len(train_data_neg)):]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_pos)"
      ],
      "metadata": {
        "id": "HnCqRe7VN8Xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c542c966-6a5a-4302-d632-a3beb1228a4c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "R8aKuguU8TiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f44070a-1fca-4cf1-b83a-ebe7fe9732c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ],
      "source": [
        "# Let's see how many reviews we have in total for training\n",
        "# We should get 2000 if you kept the sample_number=1000\n",
        "# print(X_train_preprocessed.shape)\n",
        "print(len(X_train_preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjZTpu2FKwk"
      },
      "source": [
        "## Compute Word2Vec vectors on all reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Kl1V-yyBFGti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fded1aee-ed56-4df3-e935-4ee677e6af0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute phrases begin\n",
            "Compute phrases end\n"
          ]
        }
      ],
      "source": [
        "# compute bigrams, meaning detect phrases in the texts\n",
        "# For example: [\"new\",\"york\"] will be detected as one phrase \"new york\"\n",
        "print(\"Compute phrases begin\")\n",
        "bigrams = Phrases(sentences=all_reviews)\n",
        "# compute trigrams, meaning we detect three words that usually appear\n",
        "# together. Notice that because we work with a small subset, we might\n",
        "# not detect a lot of trigrams\n",
        "trigrams = Phrases(sentences=bigrams[all_reviews])\n",
        "print(\"Compute phrases end\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rRepsD6YCnCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92c1934-9b4f-4a6c-9704-9a5b33e2470e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'station', 'near', 'the', 'solar', 'system']\n"
          ]
        }
      ],
      "source": [
        "# Test how our phrase looks after calling the bigrams\n",
        "print(bigrams['space station near the solar system'.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_1x4MR6Iv1UW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38e9c12-6551-406a-e14b-1980c8a68454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['space', 'station', 'near', 'the', 'solar', 'system']\n"
          ]
        }
      ],
      "source": [
        "# Test how our phrase looks after calling the trigrams\n",
        "# Do you notice any difference compared with the bigrams?\n",
        "print(trigrams[bigrams['space station near the solar system'.split()]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "no71z1UvDFBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b75be7-44eb-45aa-ff38-3488f5bae9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start learning the word embedding\n",
            "Done learning\n"
          ]
        }
      ],
      "source": [
        "# compute word embedding from the dataset\n",
        "### Begin your code here\n",
        "# set the embedding vector size variable to 256\n",
        "embedding_vector_size = 256\n",
        "\n",
        "\n",
        "### End your code here\n",
        "\n",
        "# Next, we train a custom word2vec model based on our custom dataset.\n",
        "# Notice that the input sentences are the trigrams\n",
        "# In this case, we consider grouping like new_york one word.\n",
        "# The input of the word2vec will be the processed words as trigrams.\n",
        "# The duration of this process depends on the size of the dataset.\n",
        "# For the restricted size of all reviews, this would take around 1-2minutes\n",
        "print(\"Start learning the word embedding\")\n",
        "trigram_model = Word2Vec(\n",
        "    sentences = trigrams[bigrams[all_reviews]],\n",
        "    vector_size = embedding_vector_size,\n",
        "    min_count=3, window=5, workers=4)\n",
        "print(\"Done learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AcRcTpIJhXv"
      },
      "source": [
        "Check what is the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1SKWyBnZD7Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a0e225-f606-40d1-c0e1-6466720a96e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 13533\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocabulary size:\", len(trigram_model.wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2eqqZpSJ5eH"
      },
      "source": [
        "Let's check the most similar words for \"movie\" & \"galaxy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gEQ3aXWZEBDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a32e8d1-1a64-48c1-dc7f-b9e22dc1f25a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('scientist', 0.999759316444397),\n",
              " ('boat', 0.9997497797012329),\n",
              " ('n', 0.9997242093086243),\n",
              " ('board', 0.9997135996818542),\n",
              " ('girlfriend', 0.9997103214263916),\n",
              " ('g', 0.9997051954269409),\n",
              " ('road', 0.9997047185897827),\n",
              " ('fire', 0.999704122543335),\n",
              " ('trip', 0.9997013211250305),\n",
              " ('priest', 0.9997006058692932)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "trigram_model.wv.most_similar('sun')\n",
        "# If you are working with the subset of 1000 reviews, the most similar words might not be\n",
        "# the most relevant ones. You can remove the constraint of working with only 1000 reviews,\n",
        "# and compare what are the most similar words again, but please be aware that this might\n",
        "# increase the training time of the word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jH2o-1CXtGgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccf2d19-da8d-4c6c-e04b-247502bc3809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low_budget', 0.9994406700134277),\n",
              " ('nice', 0.9993757009506226),\n",
              " ('truly', 0.9993634223937988),\n",
              " ('fact', 0.9992556571960449),\n",
              " ('idea', 0.9992082715034485),\n",
              " ('although', 0.9991889595985413),\n",
              " ('predictable', 0.9991137981414795),\n",
              " ('one_best', 0.999112069606781),\n",
              " ('ok', 0.9990673065185547),\n",
              " ('felt', 0.9988930821418762)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "trigram_model.wv.most_similar('action')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulf_As1CKO88"
      },
      "source": [
        "Given a list of words identify which word does not match with the others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9-7HRVEniDaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f1ebc11-0544-4b49-b487-863e4c328fa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'moon'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "trigram_model.wv.doesnt_match(['moon', 'sun', 'planet'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRxTbMIXKaza"
      },
      "source": [
        "# Transform our reviews from the training set into vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "epORdcayEJDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f4a235-98e6-4e48-d9c3-29384c6eb565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert sentences to sentences with ngrams... (done)\n",
            "Vectorize sentences... (done)\n",
            "Transform sentences to sequences on the train set... (done)\n",
            "Vectorize sentences... (done)\n",
            "Transform sentences to sequences on the test set... (done)\n"
          ]
        }
      ],
      "source": [
        "def vectorize_data(data, vocab: dict) -> list:\n",
        "    print('Vectorize sentences...', end='\\r')\n",
        "    keys = list(vocab.keys())\n",
        "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "    vectorized = list(map(encode, data))\n",
        "    print('Vectorize sentences... (done)')\n",
        "    return vectorized\n",
        "\n",
        "print('Convert sentences to sentences with ngrams...', end='\\r')\n",
        "X_data = trigrams[bigrams[X_train_preprocessed]]\n",
        "print('Convert sentences to sentences with ngrams... (done)')\n",
        "input_length = 150\n",
        "\n",
        "# Transform all sequences to 150, sequences shorter are padded, while sequences longer are truncated to maximum size\n",
        "X_pad_train = pad_sequences(\n",
        "    sequences=vectorize_data(X_data, vocab=trigram_model.wv.key_to_index),\n",
        "    maxlen=input_length,\n",
        "    padding='post')\n",
        "print('Transform sentences to sequences on the train set... (done)')\n",
        "\n",
        "\n",
        "X_data_test = trigrams[bigrams[X_test_preprocessed]]\n",
        "X_pad_test = pad_sequences(\n",
        "    sequences=vectorize_data(X_data_test, vocab=trigram_model.wv.key_to_index),\n",
        "    maxlen=input_length,\n",
        "    padding='post')\n",
        "\n",
        "print('Transform sentences to sequences on the test set... (done)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zy4ZO3TrhXO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8903418e-d73a-4bc6-ff95-a8aef4ed93d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  299,     5,  2048,   226,   758,  3138,  6087,  1364,  2291,\n",
              "        6291,   603,   972,  1681,   230,  6011,    97,   220,   607,\n",
              "        9976,   269,   119,  1515,    74,   179,   706,    13,   607,\n",
              "         168,    60,  4835,  2006,   440,  1160,   648,    97,  7959,\n",
              "        5303,     0,    23,   883,  1158,  2371,   806,   260,   265,\n",
              "        4645,   616,    74,   149,  2307,    13,   697,   991,  1551,\n",
              "         130,  4598,  2371,  1158,  3390,    31,    12, 11399,    72,\n",
              "         351,  6011,   209,  4835,    31,   250,  5303,   265, 11343,\n",
              "        1935,    91,   541,    51,  1350,   114,    54,   354,  1149,\n",
              "        2351,   323,  6940,  1157,   559,     6,  5303,  6117,   345,\n",
              "        7182,     0,    10,   473,  7088,  9377,  9976,    68,  4116,\n",
              "          65,   260,  2695,  7946,  2505,  6013,   126,  3574,   109,\n",
              "         143,    45,  8070,    36,   183,   459,  2990,  1166,     1,\n",
              "          23,   836,    45,   583,    46,  2512, 11371,  1055,  7784,\n",
              "         172,  6291,  2690,   201,  1351,  7833,    11,     4,  5857,\n",
              "         704,  3056,   197,  6291,    10,  6265,  5303,     0,    20,\n",
              "         252,   496,    59,   697,  1424,    44], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# For a given example, each number in the vector represents the position of the word in the vocabulary\n",
        "X_pad_train[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AN9nzI8L_ET"
      },
      "source": [
        "# Train a classifier based on a particular type of recurrent neural network called LSTM to differentiate between positive and negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bzUtHSypGPLq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#We'll train a model based on a subset of the training set\n",
        "\n",
        "# Step 1: Split into 80% train+validation and 20% validation\n",
        "\n",
        "# BEGIN YOUR CODE HERE\n",
        "# Use train_test_split to split the dataset (X_pad and data_train['label']) into X_train, X_val, y_train, y_val.\n",
        "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "# Ensure that the data is shuffled to avoid any ordering bias, and set a random state for reproducibility.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_pad_train,\n",
        "    data_train['label'],\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "# Step 2: The test set is just the padded test sequences along with the test labels\n",
        "\n",
        "X_test = X_pad_test\n",
        "y_test = data_test['label']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6p5Jw5ju4H",
        "outputId": "789e116d-96ec-4940-b166-d866a03daf70"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1600, 150)\n",
            "X_val shape: (400, 150)\n",
            "y_train shape: (1600,)\n",
            "y_val shape: (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "_E7PFRSS30ar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "2b10fdc2-332b-42c1-c70a-27bf64d3ad99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       1\n",
              "       ..\n",
              "1995    0\n",
              "1996    0\n",
              "1997    0\n",
              "1998    0\n",
              "1999    0\n",
              "Name: label, Length: 2000, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "data_test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbChe-TENItQ"
      },
      "source": [
        "Define the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q7SQhK9F3Zp"
      },
      "outputs": [],
      "source": [
        "### Begin your code here\n",
        "# Define a neural network model\n",
        "# TIP: Use the same sequential model in order to define the network:\n",
        "# https://www.tensorflow.org/guide/keras/sequential_model\n",
        "# You can also see an example in the MNIST lab.\n",
        "# Add the following layers:\n",
        "# 1. an Embedding layer of the following form:\n",
        "# tf.keras.layers.Embedding(\n",
        "#         input_dim = trigram_model.wv.vectors.shape[0],\n",
        "#         output_dim = trigram_model.wv.vectors.shape[1],\n",
        "#         input_length = input_length,\n",
        "#         weights = [trigram_model.wv.vectors],\n",
        "#         trainable=False)\n",
        "# 2. A Bidirectional layer with LSTM, with 128 internal units and a recurrent dropout of 0.1\n",
        "# A sentence can be considered a temporal sequence, where the order of the words\n",
        "# might be important. This is why we need a temporal model, like a Long short-term memory model.\n",
        "# A bidirectional model, simply means that the we want to parse the data both forward\n",
        "# and backwards. This allows the network to capture both past and future context for each time step.\n",
        "# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "# eg: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, recurrent_dropout=0.1)),\n",
        "# 3. A dropout layer with 0.25 probability\n",
        "# You will find an example of dropout layer in the previous MNIST lab.\n",
        "# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
        "# 4. A Dense layer with 64 internal units\n",
        "# You will find an example of a dense layer in the previous MNIST lab.\n",
        "# 5. A dropout layer with 0.3 probability\n",
        "# 6. A final dense layer with 1 neuron and a sigmoid activation function\n",
        "# tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End your code here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4UyMUAxPmp5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91DuUpuBHJUn"
      },
      "outputs": [],
      "source": [
        "### Begin your code here\n",
        "# Train the model with two epochs, and a batch size of 100.\n",
        "# Tip: The x is X_train, y is y_train, and validation_data is (X_val, y_val)\n",
        "# To view the model.fit function definition check here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "# and also an example here: https://www.tensorflow.org/guide/keras/training_with_built_in_methods (Look for fit() )\n",
        "# To obtain a better accuracy you would need to train for more epochs. Find a right balance between the training time\n",
        "# and the accuracy\n",
        "# The parameters that you need to set are:\n",
        "# x as X_train\n",
        "# y as y_train\n",
        "# validation_data with the (X_val, y_val)\n",
        "# Choose and appropriate batch_size. You can experiment with different values and see how the model behaves.\n",
        "# When you first start training, only train for 1-2 epoch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the loss values for each epoch and display it in a figure\n",
        "# BEGIN YOUR CODE HERE\n",
        "\n",
        "# Extract loss values for training and validation from the history object\n",
        "# Create an array, epochs, containing integers from 1 to the number of epochs (inclusive).\n",
        "\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "\n",
        "\n",
        "\n",
        "# Add labels, title, grid, and legend\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "\n",
        "\n",
        "\n",
        "# END YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WHc-p2qieW2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNqB9Ufej6eG"
      },
      "outputs": [],
      "source": [
        "# BEGIN YOUR CODE HERE\n",
        "# What is the loss and accuracy on the Testing dataset?\n",
        "# Tip: instead of (x_test, y_test) we used in the lab last week, you can use\n",
        "# directly test_ds which contains both data and labels\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
        "# When you print the output of the evaluate function, it will return both\n",
        "# the loss and accuracy, maybe in a  format like [loss_value, accuracy_value]\n",
        "\n",
        "\n",
        "\n",
        "### End your code here\n",
        "\n",
        "# What is the accuracy you get?\n",
        "# If you get an accuracy of aprox 50%, what does it mean? Did your model learn?\n",
        "# Try to modify the architecture; how you train the model or how much data you\n",
        "# use to train it in order to improve the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnpIBL-kO6Hp"
      },
      "source": [
        "Test the model with a random text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSS9eiaLO9Y7"
      },
      "outputs": [],
      "source": [
        "test_samples = []\n",
        "\n",
        "# normally in python we have one line per instruction. Defining a long string\n",
        "# will be difficult to read if it is only in one line. The way that we tell\n",
        "# the interpreter that we have an instruction that spans several lines\n",
        "# is by using the character \\\n",
        "review1 = \"Petter Mattei's 'Love in the Time of Money' is a visually stunning\"\\\n",
        "          \"film to watch. Mr. Mattei offers us a vivid portrait about human\" \\\n",
        "          \" This is a movie that seems to be telling us what money, power and\" \\\n",
        "          \"success do to people\"\n",
        "### Begin your code here\n",
        "# Write a couple of reviews and analyse how the model performs on your own data.\n",
        "# Are the results what you expect?\n",
        "# What did you change in the network architecture to get better results?\n",
        "review2 = \"\"\n",
        "review3 = \"\"\n",
        "### End your code here\n",
        "\n",
        "test_samples.append(review1)\n",
        "test_samples.append(review2)\n",
        "test_samples.append(review3)\n",
        "\n",
        "# test_samples_preprocess = np.array(list(map(lambda x: preprocess(x, len(test_samples)), test_samples)))\n",
        "test_samples_preprocess = list(map(lambda x: preprocess(x, len(test_samples)), test_samples))\n",
        "print(test_samples_preprocess)\n",
        "print(trigrams[bigrams[test_samples_preprocess]])\n",
        "test_data_bigrams = trigrams[bigrams[test_samples_preprocess]]\n",
        "test_data_pad = pad_sequences(\n",
        "sequences=vectorize_data(test_data_bigrams, vocab=trigram_model.wv.key_to_index),\n",
        "maxlen=input_length,\n",
        "padding='post')\n",
        "\n",
        "predictions = model.predict(test_data_pad)\n",
        "#print(predictions)\n",
        "\n",
        "for (t, p) in zip( test_samples, predictions):\n",
        "    prediction_string = \"positive\"\n",
        "    if p<0.5:\n",
        "        prediction_string = \"negative\"\n",
        "    print(\"Predicted \"+prediction_string+\" \"+str(p)+\" for review:\"+ t)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}