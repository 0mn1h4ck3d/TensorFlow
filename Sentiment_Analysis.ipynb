{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tr41z/machine-learning/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers to questions in brief\n",
        "\n",
        "### **1. Dataset Analysis**\n",
        "- How many texts are present in the training set for positive sentiment cases?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- How many are there for negative sentiment cases?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- How many texts are present in the testing set for both positive and negative sentiment cases?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Is the dataset balanced between positive and negative sentiment cases?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **2. Text Example Display**\n",
        "- Using the test dataset, display one example each from the positive and negative sentiment categories.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **3. Preprocessing and Tokenization**\n",
        "- What is the maximum length to which each text should be padded/truncated for this task?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Use tf.keras.preprocessing.text.Tokenizerto fit on the training data, then convert the texts to sequences.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Implement padding and truncation on sequences using tf.keras.preprocessing.sequence.pad_sequences.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **4. Model Configuration -Batch Size and Embedding Dimension**\n",
        "- What is an appropriate batch size for this model?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Set an appropriate embedding dimension size for the word embeddings in the model.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **5. Dataset Loading**\n",
        "- Split the data into training and validation sets. Specify the validation split ratio and ensure shuffling before training.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Convert the text data into a format suitable for the model (e.g., padded sequences for a recurrent neural network).\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **6. Model Architecture**\n",
        "- How many classes are in this classification problem? Set this as num_classes.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Design a neural network model using tf.keras.Sequentialfor the sentiment analysis task.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- What layers should the model include? Use an Embedding layer at the start, followed by either an LSTM or GRU layer, and end with a Dense layer set to num_classesoutputs.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Compile the model by specifying an optimiser, a loss function, and the accuracy metric.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Test the model with and without dropout layers. What accuracy do you achieve with each setup?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **7. Model Training**\n",
        "- Train the model on the dataset for oneepoch. What is the resulting training accuracy?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- If you increase the number of epochs, how does the accuracy change?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **8. Loss Analysis and Plotting**\n",
        "- Extract and plot the training and validation loss values over each epoch.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Include labels, a title, a grid, and a legend for clarity in the loss plot.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **9. Testing Dataset Evaluation**\n",
        "- Using model.evaluate, calculate and print the loss and accuracy on the testing dataset.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- What accuracy do you observe on the testing dataset?\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **10. AIM**\n",
        "- Iteratively improve parameters such as the number of layers, the type of recurrent layer (LSTM vs. GRU), and the number of epochs to achieve at least 80% accuracy on the testing set. Save the results of your top three models and explain their differences.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "### **11. Optional Challenges**\n",
        "- Given the dataset, is accuracy the best metric to use? Propose and test different metrics, such as precision, recall, and F1-score.\n",
        "\n",
        "  `ANSWER`\n",
        "\n",
        "- Explore transfer learning with pre-trained embeddings like GloVe or Word2Vec to improve the results, and/or experiment with data augmentation techniques for text data, such as synonym replacement.\n",
        "\n",
        "  `ANSWER`\n",
        "  "
      ],
      "metadata": {
        "id": "nflGIx0fjUlb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI5VD13M8fY6"
      },
      "source": [
        "# Get Data and import libraries that we need"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version of this jupyter notebook: sentimentanalysis_V2.4"
      ],
      "metadata": {
        "id": "eMwtYRl71geH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdLsVQjs8R-X"
      },
      "outputs": [],
      "source": [
        "# Define where we will download the data\n",
        "path_data = \"/content/sample_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_sKSOI8uZJ"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSPQApSIqEL9"
      },
      "outputs": [],
      "source": [
        "!wget -P /content/sample_data/ -c \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Ka_9Fk80iw"
      },
      "source": [
        "Decompress the archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyPwL6EYq-ej"
      },
      "outputs": [],
      "source": [
        "!tar -xf  /content/sample_data/aclImdb_v1.tar.gz -C /content/sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mksTVcHZ9MQ-"
      },
      "source": [
        "Check that the folder aclImdb exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt0S2EZErSi0"
      },
      "outputs": [],
      "source": [
        "!ls /content/sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZpg6f1u9W76"
      },
      "source": [
        "Import all required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bZ5Cng09LEl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wytbs1XU9zgG"
      },
      "source": [
        "Define function that reads all the files from a given folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEpWeGRq2QYd"
      },
      "outputs": [],
      "source": [
        "def read_data(path, files):\n",
        "  data = []\n",
        "  for f in files:\n",
        "    with open(path+f) as file:\n",
        "      ### BEGIN YOUR CODE HERE\n",
        "      ## Read a line from the file and append it to the data list variable.\n",
        "      ## TIP: use function append(): https://www.w3schools.com/python/ref_list_append.asp\n",
        "      ## and readline(): https://www.w3schools.com/python/ref_file_readline.asp\n",
        "\n",
        "\n",
        "      ### END YOUR CODE HERE\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zF9Dz-E-NQv"
      },
      "source": [
        "Load data (movie reviews), both for train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRmD7SSBrr71"
      },
      "outputs": [],
      "source": [
        "path_data_train_pos = path_data + '/aclImdb/train/pos/'\n",
        "path_data_train_neg = path_data + '/aclImdb/train/neg/'\n",
        "path_data_test_pos = path_data + '/aclImdb/test/pos/'\n",
        "path_data_test_neg = path_data + '/aclImdb/test/neg/'\n",
        "\n",
        "# Get the list of files from the four folders\n",
        "train_pos_files = os.listdir(path_data_train_pos)\n",
        "train_neg_files = os.listdir(path_data_train_neg)\n",
        "test_pos_files = os.listdir(path_data_test_pos)\n",
        "test_neg_files = os.listdir(path_data_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-OHWFG92TV_"
      },
      "outputs": [],
      "source": [
        "### BEGIN YOUR CODE HERE\n",
        "## Read the review data in these four variables: train_data_pos, train_data_neg,\n",
        "## test_data_pos and test_data_neg using the function defined above read_data().\n",
        "## Tip: First argument is the folder containing the files, second argument is\n",
        "# the list of files\n",
        "\n",
        "\n",
        "# 1a. How many examples do we have in training for positive reviews? How many for negative review?\n",
        "\n",
        "\n",
        "# 1b. How about in the testing set?\n",
        "\n",
        "\n",
        "\n",
        "# 1c. Is the dataset balanced or not?\n",
        "\n",
        "# 2.Print examples of positive and negative reviews from training and testing dataset\n",
        "# Tip: the output of the read_data() function is a list.\n",
        "# https://www.w3schools.com/python/python_lists_access.asp\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONyrIyabhfx_"
      },
      "outputs": [],
      "source": [
        "# Let's work on a subset of training and testing dataset to start with.\n",
        "# I recommend that while you are developing the code to work with a small number of examples, maybe 1000.\n",
        "# This will speed up how fast you can get the actual results. After the code is working, for the full tests\n",
        "# please use the entire dataset\n",
        "\n",
        "### BEGIN YOUR CODE HERE\n",
        "sample_number = TODO\n",
        "### END YOUR CODE HERE\n",
        "train_data_pos = train_data_pos[:sample_number]\n",
        "train_data_neg = train_data_neg[:sample_number]\n",
        "test_data_pos = test_data_pos[:sample_number]\n",
        "test_data_neg = test_data_neg[:sample_number]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSNchHyBSiP"
      },
      "source": [
        "Create the data structures that we'll use for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iRunxcO214k"
      },
      "outputs": [],
      "source": [
        "### BEGIN YOUR CODE HERE\n",
        "# Assign the length of the train_data_pos to variable length_train_pos\n",
        "# Tip: to get the length of an array, you can use the function len()\n",
        "\n",
        "\n",
        "# Assign the length of the train_data_neg to variable length_train_neg\n",
        "\n",
        "\n",
        "# Assign the length of the test_data_pos to variable length_test_pos\n",
        "\n",
        "\n",
        "# Assign the length of the test_data_neg to variable length_test_neg\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE HERE\n",
        "\n",
        "print(\"Length of the positive training examples is\", length_train_pos )\n",
        "print(\"Length of the negative training examples is\", length_train_neg)\n",
        "print(\"Length of the positive testing examples is\", length_test_pos )\n",
        "print(\"Length of the negative testing examples is\", length_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP0iN7Y63DK8"
      },
      "outputs": [],
      "source": [
        "# Create the training DataFrame with examples and labels\n",
        "# Concatenate positive and negative training examples, then pair each example with a label: 1 for positive, 0 for negative\n",
        "data_train = pd.DataFrame(zip(train_data_pos+train_data_neg, [1]*length_train_pos+[0]*length_train_neg),  columns=['review', 'label'])\n",
        "# Create the test DataFrame with examples and labels\n",
        "# Concatenate positive and negative test examples, then pair each example with a label: 1 for positive, 0 for negative\n",
        "data_test = pd.DataFrame(zip(test_data_pos+test_data_neg, [1]*length_test_pos+[0]*length_test_pos),  columns=['review', 'label'])\n",
        "# Combine all reviews into a single list (training + test, positive + negative)\n",
        "all_reviews = train_data_pos+train_data_neg+test_data_pos+test_data_neg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AgnWZ6xmPKC"
      },
      "outputs": [],
      "source": [
        "print(\"The length of the train reviews is\",len(data_train))\n",
        "print(\"The length of the test reviews is\",len(data_test))\n",
        "print(\"The length of all reviews is\",len(all_reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGx0YdcMDKmk"
      },
      "source": [
        "## The following are functions for preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kUEiSjwDWtq"
      },
      "outputs": [],
      "source": [
        "# Download stopwords and wordnet vectors\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "counter = 0\n",
        "REPLACE_WITH_SPACE = re.compile(r'[^A-Za-z\\s]')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "# Declare the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upAJm0cL4m55"
      },
      "outputs": [],
      "source": [
        "# The following functions preprocess text data\n",
        "def clean_review(raw_review: str) -> str:\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(str(raw_review), \"lxml\").get_text()\n",
        "    # 2. Remove non-letters\n",
        "    letters_only = REPLACE_WITH_SPACE.sub(\" \", review_text)\n",
        "    # 3. Convert to lower case\n",
        "    lowercase_letters = letters_only.lower()\n",
        "    return lowercase_letters\n",
        "\n",
        "\n",
        "def lemmatize(tokens: list) -> list:\n",
        "    # 1. Lemmatize\n",
        "    tokens = list(map(lemmatizer.lemmatize, tokens))\n",
        "    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n",
        "    # 2. Remove stop words\n",
        "    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n",
        "    return meaningful_words\n",
        "\n",
        "\n",
        "def preprocess(review: str, total: int, show_progress: bool = True) -> list:\n",
        "    if show_progress:\n",
        "        global counter\n",
        "        counter += 1\n",
        "        print('Processing... %6i/%6i'% (counter, total), end='\\r')\n",
        "    # 1. Clean text\n",
        "    review = clean_review(review)\n",
        "    # 2. Split into individual words\n",
        "    tokens = word_tokenize(review)\n",
        "    # 3. Lemmatize\n",
        "    lemmas = lemmatize(tokens)\n",
        "    # 4. Join the words back into one string separated by space,\n",
        "    # and return the result.\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZheAzAmxEIRi"
      },
      "source": [
        "## Preprocess the text of the reviews by removing the non-word characters, converting everything to lower case, and lemmatizing words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qCpWwk299fJ"
      },
      "outputs": [],
      "source": [
        "### Begin your code here\n",
        "\n",
        "# Display the first review text from the training data.\n",
        "# Purpose: To check how the raw data looks before applying any preprocessing.\n",
        "# Tip: The training data is stored in a pandas DataFrame (2-dimensional array-like structure).\n",
        "# Here, we access the 'review' column of the first row.\n",
        "\n",
        "# Display the label associated with the first review in the training data.\n",
        "# Purpose: To confirm the label for the first review, indicating its classification (1 for positive, 0 for negative).\n",
        "\n",
        "# Display the preprocessed text of the first review.\n",
        "# Purpose: To see the effect of preprocessing on the raw text.\n",
        "# Tip: Use the preprocess() function, passing the first review text and specifying \"1\" as the second argument\n",
        "# since we're processing a single example.\n",
        "\n",
        "### End your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8v52gEBEtUi"
      },
      "source": [
        "Let's preproces the entire set of reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhjaSx_R_QUL"
      },
      "outputs": [],
      "source": [
        "# Preprocess each review in all_reviews, applying the preprocess function to each item.\n",
        "# The preprocess function  takes each review (x) and the total number of reviews (len(all_reviews))\n",
        "# as inputs, performing some transformation or cleaning on each review based on the dataset size.\n",
        "\n",
        "all_reviews = [preprocess(x, len(all_reviews)) for x in all_reviews]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o9C3aCI8TiF"
      },
      "outputs": [],
      "source": [
        "# Slice the preprocessed reviews to get only the training set portion (first len(train_data_pos) + len(train_data_neg) items)\n",
        "X_train_preprocessed = all_reviews[:(len(train_data_pos)+len(train_data_neg))]\n",
        "X_test_preprocessed = all_reviews[(len(train_data_pos)+len(train_data_neg)):]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_pos)"
      ],
      "metadata": {
        "id": "HnCqRe7VN8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8aKuguU8TiI"
      },
      "outputs": [],
      "source": [
        "# Let's see how many reviews we have in total for training\n",
        "# We should get 2000 if you kept the sample_number=1000\n",
        "# print(X_train_preprocessed.shape)\n",
        "print(len(X_train_preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjZTpu2FKwk"
      },
      "source": [
        "## Compute Word2Vec vectors on all reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl1V-yyBFGti"
      },
      "outputs": [],
      "source": [
        "# compute bigrams, meaning detect phrases in the texts\n",
        "# For example: [\"new\",\"york\"] will be detected as one phrase \"new york\"\n",
        "print(\"Compute phrases begin\")\n",
        "bigrams = Phrases(sentences=all_reviews)\n",
        "# compute trigrams, meaning we detect three words that usually appear\n",
        "# together. Notice that because we work with a small subset, we might\n",
        "# not detect a lot of trigrams\n",
        "trigrams = Phrases(sentences=bigrams[all_reviews])\n",
        "print(\"Compute phrases end\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRepsD6YCnCK"
      },
      "outputs": [],
      "source": [
        "# Test how our phrase looks after calling the bigrams\n",
        "print(bigrams['space station near the solar system'.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1x4MR6Iv1UW"
      },
      "outputs": [],
      "source": [
        "# Test how our phrase looks after calling the trigrams\n",
        "# Do you notice any difference compared with the bigrams?\n",
        "print(trigrams[bigrams['space station near the solar system'.split()]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no71z1UvDFBn"
      },
      "outputs": [],
      "source": [
        "# compute word embedding from the dataset\n",
        "### Begin your code here\n",
        "# set the embedding vector size variable to 256\n",
        "\n",
        "\n",
        "\n",
        "### End your code here\n",
        "\n",
        "# Next, we train a custom word2vec model based on our custom dataset.\n",
        "# Notice that the input sentences are the trigrams\n",
        "# In this case, we consider grouping like new_york one word.\n",
        "# The input of the word2vec will be the processed words as trigrams.\n",
        "# The duration of this process depends on the size of the dataset.\n",
        "# For the restricted size of all reviews, this would take around 1-2minutes\n",
        "print(\"Start learning the word embedding\")\n",
        "trigram_model = Word2Vec(\n",
        "    sentences = trigrams[bigrams[all_reviews]],\n",
        "    vector_size = embedding_vector_size,\n",
        "    min_count=3, window=5, workers=4)\n",
        "print(\"Done learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AcRcTpIJhXv"
      },
      "source": [
        "Check what is the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SKWyBnZD7Ig"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary size:\", len(trigram_model.wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2eqqZpSJ5eH"
      },
      "source": [
        "Let's check the most similar words for \"movie\" & \"galaxy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEQ3aXWZEBDn"
      },
      "outputs": [],
      "source": [
        "trigram_model.wv.most_similar('sun')\n",
        "# If you are working with the subset of 1000 reviews, the most similar words might not be\n",
        "# the most relevant ones. You can remove the constraint of working with only 1000 reviews,\n",
        "# and compare what are the most similar words again, but please be aware that this might\n",
        "# increase the training time of the word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH2o-1CXtGgk"
      },
      "outputs": [],
      "source": [
        "trigram_model.wv.most_similar('action')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulf_As1CKO88"
      },
      "source": [
        "Given a list of words identify which word does not match with the others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-7HRVEniDaO"
      },
      "outputs": [],
      "source": [
        "trigram_model.wv.doesnt_match(['moon', 'sun', 'planet'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRxTbMIXKaza"
      },
      "source": [
        "# Transform our reviews from the training set into vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epORdcayEJDx"
      },
      "outputs": [],
      "source": [
        "def vectorize_data(data, vocab: dict) -> list:\n",
        "    print('Vectorize sentences...', end='\\r')\n",
        "    keys = list(vocab.keys())\n",
        "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "    vectorized = list(map(encode, data))\n",
        "    print('Vectorize sentences... (done)')\n",
        "    return vectorized\n",
        "\n",
        "print('Convert sentences to sentences with ngrams...', end='\\r')\n",
        "X_data = trigrams[bigrams[X_train_preprocessed]]\n",
        "print('Convert sentences to sentences with ngrams... (done)')\n",
        "input_length = 150\n",
        "\n",
        "# Transform all sequences to 150, sequences shorter are padded, while sequences longer are truncated to maximum size\n",
        "X_pad_train = pad_sequences(\n",
        "    sequences=vectorize_data(X_data, vocab=trigram_model.wv.key_to_index),\n",
        "    maxlen=input_length,\n",
        "    padding='post')\n",
        "print('Transform sentences to sequences on the train set... (done)')\n",
        "\n",
        "\n",
        "X_data_test = trigrams[bigrams[X_test_preprocessed]]\n",
        "X_pad_test = pad_sequences(\n",
        "    sequences=vectorize_data(X_data_test, vocab=trigram_model.wv.key_to_index),\n",
        "    maxlen=input_length,\n",
        "    padding='post')\n",
        "\n",
        "print('Transform sentences to sequences on the test set... (done)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy4ZO3TrhXO8"
      },
      "outputs": [],
      "source": [
        "# For a given example, each number in the vector represents the position of the word in the vocabulary\n",
        "X_pad_train[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AN9nzI8L_ET"
      },
      "source": [
        "# Train a classifier based on a particular type of recurrent neural network called LSTM to differentiate between positive and negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzUtHSypGPLq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#We'll train a model based on a subset of the training set\n",
        "\n",
        "# Step 1: Split into 80% train+validation and 20% validation\n",
        "\n",
        "# BEGIN YOUR CODE HERE\n",
        "# Use train_test_split to split the dataset (X_pad and data_train['label']) into X_train, X_val, y_train, y_val.\n",
        "# https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "# Ensure that the data is shuffled to avoid any ordering bias, and set a random state for reproducibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "# Step 2: The test set is just the padded test sequences along with the test labels\n",
        "\n",
        "X_test = X_pad_test\n",
        "y_test = data_test['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E7PFRSS30ar"
      },
      "outputs": [],
      "source": [
        "data_test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbChe-TENItQ"
      },
      "source": [
        "Define the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q7SQhK9F3Zp"
      },
      "outputs": [],
      "source": [
        "### Begin your code here\n",
        "# Define a neural network model\n",
        "# TIP: Use the same sequential model in order to define the network:\n",
        "# https://www.tensorflow.org/guide/keras/sequential_model\n",
        "# You can also see an example in the MNIST lab.\n",
        "# Add the following layers:\n",
        "# 1. an Embedding layer of the following form:\n",
        "# tf.keras.layers.Embedding(\n",
        "#         input_dim = trigram_model.wv.vectors.shape[0],\n",
        "#         output_dim = trigram_model.wv.vectors.shape[1],\n",
        "#         input_length = input_length,\n",
        "#         weights = [trigram_model.wv.vectors],\n",
        "#         trainable=False)\n",
        "# 2. A Bidirectional layer with LSTM, with 128 internal units and a recurrent dropout of 0.1\n",
        "# A sentence can be considered a temporal sequence, where the order of the words\n",
        "# might be important. This is why we need a temporal model, like a Long short-term memory model.\n",
        "# A bidirectional model, simply means that the we want to parse the data both forward\n",
        "# and backwards. This allows the network to capture both past and future context for each time step.\n",
        "# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "# eg: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, recurrent_dropout=0.1)),\n",
        "# 3. A dropout layer with 0.25 probability\n",
        "# You will find an example of dropout layer in the previous MNIST lab.\n",
        "# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
        "# 4. A Dense layer with 64 internal units\n",
        "# You will find an example of a dense layer in the previous MNIST lab.\n",
        "# 5. A dropout layer with 0.3 probability\n",
        "# 6. A final dense layer with 1 neuron and a sigmoid activation function\n",
        "# tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End your code here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4UyMUAxPmp5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91DuUpuBHJUn"
      },
      "outputs": [],
      "source": [
        "### Begin your code here\n",
        "# Train the model with two epochs, and a batch size of 100.\n",
        "# Tip: The x is X_train, y is y_train, and validation_data is (X_val, y_val)\n",
        "# To view the model.fit function definition check here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "# and also an example here: https://www.tensorflow.org/guide/keras/training_with_built_in_methods (Look for fit() )\n",
        "# To obtain a better accuracy you would need to train for more epochs. Find a right balance between the training time\n",
        "# and the accuracy\n",
        "# The parameters that you need to set are:\n",
        "# x as X_train\n",
        "# y as y_train\n",
        "# validation_data with the (X_val, y_val)\n",
        "# Choose and appropriate batch_size. You can experiment with different values and see how the model behaves.\n",
        "# When you first start training, only train for 1-2 epoch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the loss values for each epoch and display it in a figure\n",
        "# BEGIN YOUR CODE HERE\n",
        "\n",
        "# Extract loss values for training and validation from the history object\n",
        "# Create an array, epochs, containing integers from 1 to the number of epochs (inclusive).\n",
        "\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "\n",
        "\n",
        "\n",
        "# Add labels, title, grid, and legend\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "\n",
        "\n",
        "\n",
        "# END YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WHc-p2qieW2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNqB9Ufej6eG"
      },
      "outputs": [],
      "source": [
        "# BEGIN YOUR CODE HERE\n",
        "# What is the loss and accuracy on the Testing dataset?\n",
        "# Tip: instead of (x_test, y_test) we used in the lab last week, you can use\n",
        "# directly test_ds which contains both data and labels\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
        "# When you print the output of the evaluate function, it will return both\n",
        "# the loss and accuracy, maybe in a  format like [loss_value, accuracy_value]\n",
        "\n",
        "\n",
        "\n",
        "### End your code here\n",
        "\n",
        "# What is the accuracy you get?\n",
        "# If you get an accuracy of aprox 50%, what does it mean? Did your model learn?\n",
        "# Try to modify the architecture; how you train the model or how much data you\n",
        "# use to train it in order to improve the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnpIBL-kO6Hp"
      },
      "source": [
        "Test the model with a random text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSS9eiaLO9Y7"
      },
      "outputs": [],
      "source": [
        "test_samples = []\n",
        "\n",
        "# normally in python we have one line per instruction. Defining a long string\n",
        "# will be difficult to read if it is only in one line. The way that we tell\n",
        "# the interpreter that we have an instruction that spans several lines\n",
        "# is by using the character \\\n",
        "review1 = \"Petter Mattei's 'Love in the Time of Money' is a visually stunning\"\\\n",
        "          \"film to watch. Mr. Mattei offers us a vivid portrait about human\" \\\n",
        "          \" This is a movie that seems to be telling us what money, power and\" \\\n",
        "          \"success do to people\"\n",
        "### Begin your code here\n",
        "# Write a couple of reviews and analyse how the model performs on your own data.\n",
        "# Are the results what you expect?\n",
        "# What did you change in the network architecture to get better results?\n",
        "review2 = \"\"\n",
        "review3 = \"\"\n",
        "### End your code here\n",
        "\n",
        "test_samples.append(review1)\n",
        "test_samples.append(review2)\n",
        "test_samples.append(review3)\n",
        "\n",
        "# test_samples_preprocess = np.array(list(map(lambda x: preprocess(x, len(test_samples)), test_samples)))\n",
        "test_samples_preprocess = list(map(lambda x: preprocess(x, len(test_samples)), test_samples))\n",
        "print(test_samples_preprocess)\n",
        "print(trigrams[bigrams[test_samples_preprocess]])\n",
        "test_data_bigrams = trigrams[bigrams[test_samples_preprocess]]\n",
        "test_data_pad = pad_sequences(\n",
        "sequences=vectorize_data(test_data_bigrams, vocab=trigram_model.wv.key_to_index),\n",
        "maxlen=input_length,\n",
        "padding='post')\n",
        "\n",
        "predictions = model.predict(test_data_pad)\n",
        "#print(predictions)\n",
        "\n",
        "for (t, p) in zip( test_samples, predictions):\n",
        "    prediction_string = \"positive\"\n",
        "    if p<0.5:\n",
        "        prediction_string = \"negative\"\n",
        "    print(\"Predicted \"+prediction_string+\" \"+str(p)+\" for review:\"+ t)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}